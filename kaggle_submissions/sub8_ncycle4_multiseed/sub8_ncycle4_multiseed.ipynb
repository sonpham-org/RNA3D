{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Sub 8: N_CYCLE=4 + Multi-Seed + All Templates + RibonanzaNet2\n\n**Hypothesis:** N_CYCLE=4 is RNAPro's default (we've been using 10, which is 2.5x slower).\nThe default was chosen by NVIDIA as the best quality/speed tradeoff. Running at N_CYCLE=4\nfrees GPU time for multi-seed diversity (3 seeds x 5 samples = 15 candidates).\n\nStrategy:\n1. Run TBM to generate 5 templates\n2. Run RNAPro with N_CYCLE=4, template_idx=4, seeds=42,101,202, N_SAMPLE=5\n3. 2.5x faster per target enables processing more targets at full quality\n4. 15 candidates per target, best 5 selected by ranking_score"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\nIS_SCORING_RUN = os.environ.get('KAGGLE_IS_COMPETITION_RERUN')\nprint('Scoring run:', IS_SCORING_RUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section A: Setup RNAPro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/rnapro-src/RNAPro /kaggle/working/\n!cp /kaggle/input/rnapro-src/rnapro-private-best-500m.ckpt /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%cd /kaggle/working/RNAPro\n!pip install -e . --no-deps\n# Install critical RNAPro dependencies not in Kaggle base image\n!pip install biotite==1.4.0 rdkit-pypi\n%cd /kaggle/working"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-index /kaggle/input/biopython-cp312/biopython-1.86-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section B: Improved TBM (6feb_v1 + validation data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\nimport numpy as np\nimport random\nimport time\nimport warnings\nimport os, sys\n\nwarnings.filterwarnings('ignore')\n\nDATA_PATH = '/kaggle/input/stanford-rna-3d-folding-2/'\ntrain_seqs = pd.read_csv(DATA_PATH + 'train_sequences.csv')\ntest_seqs = pd.read_csv(DATA_PATH + 'test_sequences.csv')\ntrain_labels = pd.read_csv(DATA_PATH + 'train_labels.csv')\n\n# Load and merge validation data\ntry:\n    validation_seqs = pd.read_csv(DATA_PATH + 'validation_sequences.csv')\n    validation_labels = pd.read_csv(DATA_PATH + 'validation_labels.csv')\n    combined_seqs = pd.concat([train_seqs, validation_seqs], ignore_index=True)\n    print(f\"Combined: {len(combined_seqs)} template sequences (train+validation)\")\nexcept FileNotFoundError:\n    combined_seqs = train_seqs\n    validation_labels = None\n    print(\"Validation not found, using train only\")\n\nsys.path.append(os.path.join(DATA_PATH, \"extra\"))\n\ntry:\n    import typing as _typing\n    import builtins as _builtins\n    _builtins.Dict  = getattr(_typing, \"Dict\")\n    _builtins.Tuple = getattr(_typing, \"Tuple\")\n    _builtins.List  = getattr(_typing, \"List\")\n    from parse_fasta_py import parse_fasta as _parse_fasta_raw\n    def parse_fasta(fasta_content: str):\n        d = _parse_fasta_raw(fasta_content)\n        out = {}\n        for k, v in d.items():\n            out[k] = v[0] if isinstance(v, tuple) else v\n        return out\nexcept Exception:\n    def parse_fasta(fasta_content: str):\n        out = {}\n        cur = None\n        seq_parts = []\n        for line in str(fasta_content).splitlines():\n            line = line.strip()\n            if not line: continue\n            if line.startswith(\">\"):\n                if cur is not None:\n                    out[cur] = \"\".join(seq_parts)\n                cur = line[1:].split()[0]\n                seq_parts = []\n            else:\n                seq_parts.append(line.replace(\" \", \"\"))\n        if cur is not None:\n            out[cur] = \"\".join(seq_parts)\n        return out\n\ndef parse_stoichiometry(stoich: str):\n    if pd.isna(stoich) or str(stoich).strip() == \"\":\n        return []\n    out = []\n    for part in str(stoich).split(';'):\n        ch, cnt = part.split(':')\n        out.append((ch.strip(), int(cnt)))\n    return out\n\ndef get_chain_segments(row):\n    seq = row['sequence']\n    stoich = row.get('stoichiometry', '')\n    all_seq = row.get('all_sequences', '')\n    if pd.isna(stoich) or pd.isna(all_seq) or str(stoich).strip()==\"\" or str(all_seq).strip()==\"\":\n        return [(0, len(seq))]\n    try:\n        chain_dict = parse_fasta(all_seq)\n        order = parse_stoichiometry(stoich)\n        segs, pos = [], 0\n        for ch, cnt in order:\n            base = chain_dict.get(ch)\n            if base is None: return [(0, len(seq))]\n            for _ in range(cnt):\n                L = len(base)\n                segs.append((pos, pos + L))\n                pos += L\n        if pos != len(seq): return [(0, len(seq))]\n        return segs\n    except Exception:\n        return [(0, len(seq))]\n\ndef build_segments_map(df):\n    seg_map, stoich_map = {}, {}\n    for _, r in df.iterrows():\n        tid = r['target_id']\n        seg_map[tid] = get_chain_segments(r)\n        stoich_map[tid] = str(r.get('stoichiometry', '') if not pd.isna(r.get('stoichiometry', '')) else '')\n    return seg_map, stoich_map\n\ntrain_segs_map, train_stoich_map = build_segments_map(train_seqs)\ntest_segs_map, test_stoich_map = build_segments_map(test_seqs)\n\ndef process_labels(labels_df):\n    coords_dict = {}\n    prefixes = labels_df['ID'].str.rsplit('_', n=1).str[0]\n    for id_prefix, group in labels_df.groupby(prefixes):\n        coords_dict[id_prefix] = group.sort_values('resid')[['x_1', 'y_1', 'z_1']].values\n    return coords_dict\n\ntrain_coords_dict = process_labels(train_labels)\nif validation_labels is not None:\n    valid_coords_dict = process_labels(validation_labels)\n    train_coords_dict.update(valid_coords_dict)\nprint(f\"Total template structures: {len(train_coords_dict)}\")\nprint(f\"Test targets: {len(test_seqs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Align import PairwiseAligner\n\naligner = PairwiseAligner()\naligner.mode = 'global'\naligner.match_score = 2\naligner.mismatch_score = -1.6\naligner.open_gap_score   = -8\naligner.extend_gap_score = -0.4\naligner.query_left_open_gap_score  = -8\naligner.query_left_extend_gap_score = -0.4\naligner.query_right_open_gap_score = -8\naligner.query_right_extend_gap_score = -0.4\naligner.target_left_open_gap_score = -8\naligner.target_left_extend_gap_score = -0.4\naligner.target_right_open_gap_score = -8\naligner.target_right_extend_gap_score = -0.4\n\ndef find_similar_sequences(query_seq, train_seqs_df, train_coords_dict, top_n=5):\n    similar_seqs = []\n    for _, row in train_seqs_df.iterrows():\n        target_id, train_seq = row['target_id'], row['sequence']\n        if target_id not in train_coords_dict: continue\n        if abs(len(train_seq) - len(query_seq)) / max(len(train_seq), len(query_seq)) > 0.3: continue\n        raw_score = aligner.score(query_seq, train_seq)\n        normalized_score = raw_score / (2 * min(len(query_seq), len(train_seq)))\n        similar_seqs.append((target_id, train_seq, normalized_score, train_coords_dict[target_id]))\n    similar_seqs.sort(key=lambda x: x[2], reverse=True)\n    return similar_seqs[:top_n]\n\ndef adapt_template_to_query(query_seq, template_seq, template_coords):\n    alignment = next(iter(aligner.align(query_seq, template_seq)))\n    new_coords = np.full((len(query_seq), 3), np.nan)\n    for (q_start, q_end), (t_start, t_end) in zip(*alignment.aligned):\n        t_chunk = template_coords[t_start:t_end]\n        if len(t_chunk) == (q_end - q_start):\n            new_coords[q_start:q_end] = t_chunk\n    for i in range(len(new_coords)):\n        if np.isnan(new_coords[i, 0]):\n            prev_v = next((j for j in range(i-1, -1, -1) if not np.isnan(new_coords[j, 0])), -1)\n            next_v = next((j for j in range(i+1, len(new_coords)) if not np.isnan(new_coords[j, 0])), -1)\n            if prev_v >= 0 and next_v >= 0:\n                w = (i - prev_v) / (next_v - prev_v)\n                new_coords[i] = (1-w)*new_coords[prev_v] + w*new_coords[next_v]\n            elif prev_v >= 0: new_coords[i] = new_coords[prev_v] + [3, 0, 0]\n            elif next_v >= 0: new_coords[i] = new_coords[next_v] + [3, 0, 0]\n            else: new_coords[i] = [i*3, 0, 0]\n    return np.nan_to_num(new_coords)\n\ndef adaptive_rna_constraints(coordinates, target_id, confidence=1.0, passes=2):\n    coords = coordinates.copy()\n    segments = test_segs_map.get(target_id, [(0, len(coords))])\n    strength = 0.80 * (1.0 - min(confidence, 0.98))\n    strength = max(strength, 0.02)\n    for _ in range(passes):\n        for (s, e) in segments:\n            X = coords[s:e]\n            L = e - s\n            if L < 3: continue\n            # Bond i,i+1 to ~5.95A\n            d = X[1:] - X[:-1]\n            dist = np.linalg.norm(d, axis=1) + 1e-5\n            scale = (5.95 - dist) / dist\n            adj = (d * scale[:, None]) * (0.22 * strength)\n            X[:-1] -= adj; X[1:] += adj\n            # Soft i,i+2 to ~10.2A\n            d2 = X[2:] - X[:-2]\n            dist2 = np.linalg.norm(d2, axis=1) + 1e-6\n            scale2 = (10.2 - dist2) / dist2\n            adj2 = (d2 * scale2[:, None]) * (0.10 * strength)\n            X[:-2] -= adj2; X[2:] += adj2\n            # Laplacian smoothing\n            lap = 0.5 * (X[:-2] + X[2:]) - X[1:-1]\n            X[1:-1] += (0.06 * strength) * lap\n            # Self-avoidance\n            if L >= 25:\n                k = min(L, 160) if L > 220 else L\n                idx = np.linspace(0, L-1, k).astype(int) if k < L else np.arange(L)\n                P = X[idx]\n                diff = P[:, None, :] - P[None, :, :]\n                distm = np.linalg.norm(diff, axis=2) + 1e-6\n                sep = np.abs(idx[:, None] - idx[None, :])\n                mask = (sep > 2) & (distm < 3.3)\n                if np.any(mask):\n                    force = (3.3 - distm) / distm\n                    vec = (diff * force[:, :, None] * mask[:, :, None]).sum(axis=1)\n                    X[idx] += (0.015 * strength) * vec\n            coords[s:e] = X\n    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rotmat(axis, ang):\n    axis = np.asarray(axis, float)\n    axis = axis / (np.linalg.norm(axis) + 1e-12)\n    x, y, z = axis\n    c, s = np.cos(ang), np.sin(ang)\n    C = 1.0 - c\n    return np.array([[c+x*x*C, x*y*C-z*s, x*z*C+y*s],\n                     [y*x*C+z*s, c+y*y*C, y*z*C-x*s],\n                     [z*x*C-y*s, z*y*C+x*s, c+z*z*C]], dtype=float)\n\ndef apply_hinge(coords, seg, rng, max_angle_deg=25):\n    s, e = seg\n    L = e - s\n    if L < 30: return coords\n    pivot = s + int(rng.integers(10, L - 10))\n    axis = rng.normal(size=3)\n    ang = np.deg2rad(float(rng.uniform(-max_angle_deg, max_angle_deg)))\n    R = _rotmat(axis, ang)\n    X = coords.copy()\n    p0 = X[pivot].copy()\n    X[pivot+1:e] = (X[pivot+1:e] - p0) @ R.T + p0\n    return X\n\ndef jitter_chains(coords, segments, rng, max_angle_deg=12, max_trans=1.5):\n    X = coords.copy()\n    gc = X.mean(axis=0, keepdims=True)\n    for (s, e) in segments:\n        axis = rng.normal(size=3)\n        ang = np.deg2rad(float(rng.uniform(-max_angle_deg, max_angle_deg)))\n        R = _rotmat(axis, ang)\n        shift = rng.normal(size=3)\n        shift = shift / (np.linalg.norm(shift) + 1e-10) * float(rng.uniform(0.0, max_trans))\n        c = X[s:e].mean(axis=0, keepdims=True)\n        X[s:e] = (X[s:e] - c) @ R.T + c + shift\n    X -= X.mean(axis=0, keepdims=True) - gc\n    return X\n\ndef smooth_wiggle(coords, segments, rng, amp=0.8):\n    X = coords.copy()\n    for (s, e) in segments:\n        L = e - s\n        if L < 20: continue\n        ctrl_x = np.linspace(0, L-1, 6)\n        ctrl_disp = rng.normal(0, amp, size=(6, 3))\n        t = np.arange(L)\n        disp = np.vstack([np.interp(t, ctrl_x, ctrl_disp[:, k]) for k in range(3)]).T\n        X[s:e] += disp\n    return X\n\ndef predict_rna_structures(row, train_seqs_df, train_coords_dict, n_predictions=5):\n    tid = row['target_id']\n    seq = row['sequence']\n    segments = test_segs_map.get(tid, [(0, len(seq))])\n    cands = find_similar_sequences(query_seq=seq, train_seqs_df=train_seqs_df,\n                                   train_coords_dict=train_coords_dict, top_n=40)\n    predictions, used = [], set()\n    for i in range(n_predictions):\n        seed = (abs(hash(tid)) + i * 10005) % (2**32)\n        rng = np.random.default_rng(seed)\n        if not cands:\n            coords = np.zeros((len(seq), 3), dtype=float)\n            for (s, e) in segments:\n                for j in range(s+1, e): coords[j] = coords[j-1] + [5.95, 0, 0]\n            predictions.append(coords); continue\n        if i == 0:\n            t_id, t_seq, sim, t_coords = cands[0]\n        else:\n            K = min(12, len(cands))\n            sims = np.array([cands[k][2] for k in range(K)], float)\n            w = np.exp((sims - sims.max()) / 0.10)\n            for k in range(K):\n                if cands[k][0] in used: w[k] *= 0.10\n            w /= (w.sum() + 1e-10)\n            k = int(rng.choice(np.arange(K), p=w))\n            t_id, t_seq, sim, t_coords = cands[k]\n        used.add(t_id)\n        adapted = adapt_template_to_query(query_seq=seq, template_seq=t_seq, template_coords=t_coords)\n        if i == 0: X = adapted\n        elif i == 1: X = adapted + rng.normal(0, max(0.01, (0.40-sim)*0.06), adapted.shape)\n        elif i == 2:\n            longest = max(segments, key=lambda se: se[1]-se[0])\n            X = apply_hinge(adapted, longest, rng, max_angle_deg=22)\n        elif i == 3: X = jitter_chains(adapted, segments, rng, max_angle_deg=10, max_trans=1.0)\n        else: X = smooth_wiggle(adapted, segments, rng, amp=0.8)\n        predictions.append(adaptive_rna_constraints(X, tid, confidence=sim, passes=2))\n    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate TBM predictions\nall_predictions = []\nstart_time = time.time()\nfor idx, row in test_seqs.iterrows():\n    if idx % 10 == 0: print(f\"TBM: {idx}/{len(test_seqs)} | {time.time()-start_time:.1f}s\")\n    tid, seq = row['target_id'], row['sequence']\n    preds = predict_rna_structures(row, combined_seqs, train_coords_dict)\n    for j in range(len(seq)):\n        res = {'ID': f\"{tid}_{j+1}\", 'resname': seq[j], 'resid': j+1}\n        for i in range(5):\n            res[f'x_{i+1}'], res[f'y_{i+1}'], res[f'z_{i+1}'] = preds[i][j]\n        all_predictions.append(res)\n\nsub_tbm = pd.DataFrame(all_predictions)\ncols = ['ID', 'resname', 'resid'] + [f'{c}_{i}' for i in range(1,6) for c in ['x','y','z']]\ncoord_cols = [c for c in cols if c.startswith(('x_','y_','z_'))]\nsub_tbm[coord_cols] = sub_tbm[coord_cols].clip(-999.999, 9999.999)\nsub_tbm[cols].to_csv('/kaggle/working/submission_tbm.csv', index=False)\nprint(f\"TBM done in {time.time()-start_time:.1f}s, shape: {sub_tbm.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section C: Convert TBM templates to .pt for RNAPro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /kaggle/working/RNAPro\n!python preprocess/convert_templates_to_pt_files.py --input_csv /kaggle/working/submission_tbm.csv --output_name templates.pt\n%cd /kaggle/working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section D: CCD cache setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIST = \"/kaggle/working/RNAPro/release_data/ccd_cache/\"\nos.makedirs(DIST, exist_ok=True)\n!cp /kaggle/input/rnapro-ccd-cache/ccd_cache/components.cif $DIST\n!cp /kaggle/input/rnapro-ccd-cache/ccd_cache/components.cif.rdkit_mol.pkl $DIST\nprint(\"CCD cache ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section E: RNAPro Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sequences CSV (full test set for scoring run, head(5) for dev)\nimport pandas as pd\ndf = pd.read_csv(\"/kaggle/input/stanford-rna-3d-folding-2/test_sequences.csv\")\nif not IS_SCORING_RUN:\n    df = df.head(5)\ndf.to_csv('/kaggle/working/sample_sequences.csv', index=False)\nprint(f\"Prepared {len(df)} sequences for RNAPro inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%writefile /kaggle/working/RNAPro/runner/inference.py\nimport os\nimport shutil\nimport logging\nimport traceback\nimport warnings\nimport argparse\nfrom contextlib import nullcontext\nfrom os.path import join as opjoin\nfrom typing import Any, Mapping\nimport glob as glob_mod\n\nimport json\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom biotite.structure.io import pdbx\n\nfrom configs.configs_base import configs as configs_base\nfrom configs.configs_data import data_configs\nfrom configs.configs_inference import inference_configs\nfrom runner.dumper import DataDumper\n\nfrom rnapro.config import parse_sys_args\nfrom rnapro.config.config import ConfigManager, ArgumentNotSet\nfrom rnapro.data.infer_data_pipeline import get_inference_dataloader\nfrom rnapro.model.RNAPro import RNAPro\nfrom rnapro.utils.distributed import DIST_WRAPPER\nfrom rnapro.utils.seed import seed_everything\nfrom rnapro.utils.torch_utils import to_device\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.WARNING)\nlogging.getLogger(\"rnapro.data\").setLevel(logging.WARNING)\nlogging.getLogger(\"rnapro\").setLevel(logging.WARNING)\n\n\ndef parse_configs(configs, arg_str=None, fill_required_with_null=False):\n    manager = ConfigManager(configs, fill_required_with_null=fill_required_with_null)\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--max_len\", type=int, default=10000, required=False)\n    parser.add_argument(\"--n_template_combos\", type=int, default=1, required=False)\n    for key, (dtype, default_value, allow_none, required) in manager.config_infos.items():\n        parser.add_argument(\"--\" + key, type=str, default=ArgumentNotSet(), required=required)\n    parsed_args = parser.parse_args(arg_str.split()) if arg_str else parser.parse_args()\n    merged_configs = manager.merge_configs(vars(parsed_args))\n    max_len = parsed_args.max_len\n    merged_configs.max_len = max_len\n    merged_configs.n_template_combos = parsed_args.n_template_combos\n    return merged_configs\n\n\nclass dotdict(dict):\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\n    def __getattr__(self, name):\n        try: return self[name]\n        except KeyError: raise AttributeError(name)\n\n\nclass InferenceRunner(object):\n    def __init__(self, configs):\n        self.configs = configs\n        self.init_env()\n        self.init_basics()\n        self.init_model()\n        self.load_checkpoint()\n        self.init_dumper(\n            need_atom_confidence=configs.need_atom_confidence,\n            sorted_by_ranking_score=configs.sorted_by_ranking_score,\n        )\n\n    def init_env(self):\n        self.print(\n            f\"Distributed environment: world size: {DIST_WRAPPER.world_size}, \"\n            + f\"global rank: {DIST_WRAPPER.rank}, local rank: {DIST_WRAPPER.local_rank}\"\n        )\n        self.use_cuda = torch.cuda.device_count() > 0\n        if self.use_cuda:\n            self.device = torch.device(\"cuda:{}\".format(DIST_WRAPPER.local_rank))\n            os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n            all_gpu_ids = \",\".join(str(x) for x in range(torch.cuda.device_count()))\n            devices = os.getenv(\"CUDA_VISIBLE_DEVICES\", all_gpu_ids)\n            torch.cuda.set_device(self.device)\n        else:\n            self.device = torch.device(\"cpu\")\n        if self.configs.use_deepspeed_evo_attention:\n            env = os.getenv(\"CUTLASS_PATH\", None)\n            self.print(f\"env: {env}\")\n            assert env is not None\n\n    def init_basics(self):\n        self.dump_dir = self.configs.dump_dir\n        self.error_dir = opjoin(self.dump_dir, \"ERR\")\n        os.makedirs(self.dump_dir, exist_ok=True)\n        os.makedirs(self.error_dir, exist_ok=True)\n\n    def init_model(self):\n        self.model = RNAPro(self.configs).to(self.device)\n        num_params = sum(p.numel() for p in self.model.parameters())\n        self.print(f\"Total number of parameters: {num_params:,}\")\n\n    def load_checkpoint(self):\n        checkpoint_path = self.configs.load_checkpoint_path\n        if not os.path.exists(checkpoint_path):\n            raise Exception(f\"Checkpoint not found: {checkpoint_path}\")\n        self.print(f\"Loading from {checkpoint_path}\")\n        checkpoint = torch.load(checkpoint_path, self.device)\n        sample_key = list(checkpoint[\"model\"].keys())[0]\n        if sample_key.startswith(\"module.\"):\n            checkpoint[\"model\"] = {\n                k[len(\"module.\"):]: v for k, v in checkpoint[\"model\"].items()\n            }\n        self.model.load_state_dict(state_dict=checkpoint[\"model\"], strict=True)\n        self.model.eval()\n\n    def init_dumper(self, need_atom_confidence=False, sorted_by_ranking_score=True):\n        self.dumper = DataDumper(\n            base_dir=self.dump_dir,\n            need_atom_confidence=need_atom_confidence,\n            sorted_by_ranking_score=sorted_by_ranking_score,\n        )\n\n    @torch.no_grad()\n    def predict(self, data):\n        eval_precision = {\n            \"fp32\": torch.float32, \"bf16\": torch.bfloat16, \"fp16\": torch.float16,\n        }[self.configs.dtype]\n        enable_amp = (\n            torch.autocast(device_type=\"cuda\", dtype=eval_precision)\n            if torch.cuda.is_available() else nullcontext()\n        )\n        data = to_device(data, self.device)\n        with enable_amp:\n            prediction, _, _ = self.model(\n                input_feature_dict=data[\"input_feature_dict\"],\n                label_full_dict=None, label_dict=None, mode=\"inference\",\n            )\n        return prediction\n\n    def print(self, msg):\n        if DIST_WRAPPER.rank == 0: print(msg)\n\n    def update_model_configs(self, new_configs):\n        self.model.configs = new_configs\n\n\ndef update_inference_configs(configs, N_token):\n    if N_token > 3840:\n        configs.skip_amp.confidence_head = False\n        configs.skip_amp.sample_diffusion = False\n    elif N_token > 2560:\n        configs.skip_amp.confidence_head = False\n        configs.skip_amp.sample_diffusion = True\n    else:\n        configs.skip_amp.confidence_head = True\n        configs.skip_amp.sample_diffusion = True\n    return configs\n\n\ndef infer_predict(runner, configs):\n    try:\n        dataloader = get_inference_dataloader(configs=configs)\n    except Exception as e:\n        error_message = f\"{e}:\\n{traceback.format_exc()}\"\n        logger.info(error_message)\n        with open(opjoin(runner.error_dir, \"error.txt\"), \"a\") as f:\n            f.write(error_message)\n        return\n    num_data = len(dataloader.dataset)\n    for seed in configs.seeds:\n        seed_everything(seed=seed, deterministic=configs.deterministic)\n        for batch in dataloader:\n            try:\n                data, atom_array, data_error_message = batch[0]\n                sample_name = data[\"sample_name\"]\n                if len(data_error_message) > 0:\n                    with open(opjoin(runner.error_dir, f\"{sample_name}.txt\"), \"a\") as f:\n                        f.write(data_error_message)\n                    continue\n                new_configs = update_inference_configs(configs, data[\"N_token\"].item())\n                runner.update_model_configs(new_configs)\n                prediction = runner.predict(data)\n                runner.dumper.dump(\n                    dataset_name=\"\", pdb_id=sample_name, seed=seed,\n                    pred_dict=prediction, atom_array=atom_array,\n                    entity_poly_type=data[\"entity_poly_type\"],\n                )\n                torch.cuda.empty_cache()\n            except Exception as e:\n                error_message = f\"{data['sample_name']} {e}:\\n{traceback.format_exc()}\"\n                logger.info(error_message)\n                with open(opjoin(runner.error_dir, f\"{sample_name}.txt\"), \"a\") as f:\n                    f.write(error_message)\n                if hasattr(torch.cuda, \"empty_cache\"): torch.cuda.empty_cache()\n\n\ndef make_dummy_solution(valid_df):\n    solution = dotdict()\n    for i, row in valid_df.iterrows():\n        solution[row.target_id] = dotdict(\n            target_id=row.target_id, sequence=row.sequence, coord=[],\n        )\n    return solution\n\n\ndef solution_to_submit_df(solution):\n    submit_df = []\n    for k, s in solution.items():\n        df = coord_to_df(s.sequence, s.coord, s.target_id)\n        submit_df.append(df)\n    return pd.concat(submit_df)\n\n\ndef coord_to_df(sequence, coord, target_id):\n    L = len(sequence)\n    df = pd.DataFrame()\n    df[\"ID\"] = [f\"{target_id}_{i+1}\" for i in range(L)]\n    df[\"resname\"] = [s for s in sequence]\n    df[\"resid\"] = [i+1 for i in range(L)]\n    for j in range(len(coord)):\n        df[f\"x_{j+1}\"] = coord[j][:, 0]\n        df[f\"y_{j+1}\"] = coord[j][:, 1]\n        df[f\"z_{j+1}\"] = coord[j][:, 2]\n    return df\n\n\ndef create_input_json(sequence, target_id):\n    return [{\n        \"sequences\": [{\"rnaSequence\": {\"sequence\": sequence, \"count\": 1}}],\n        \"name\": target_id,\n    }]\n\n\ndef extract_c1_coordinates(cif_file_path):\n    try:\n        with open(cif_file_path, \"r\") as f:\n            cif_data = pdbx.CIFFile.read(f)\n        atom_array = pdbx.get_structure(cif_data, model=1)\n        atom_names_clean = np.char.strip(atom_array.atom_name.astype(str))\n        mask_c1 = atom_names_clean == \"C1'\"\n        c1_atoms = atom_array[mask_c1]\n        if len(c1_atoms) == 0:\n            print(f\"Warning: No C1' atoms found in {cif_file_path}\")\n            return None\n        sort_indices = np.argsort(c1_atoms.res_id)\n        return c1_atoms[sort_indices].coord\n    except Exception as e:\n        print(f\"Error extracting C1' from {cif_file_path}: {e}\")\n        return None\n\n\ndef process_sequence(sequence, target_id, temp_dir):\n    input_json = create_input_json(sequence, target_id)\n    os.makedirs(temp_dir, exist_ok=True)\n    input_json_path = os.path.join(temp_dir, f\"{target_id}_input.json\")\n    with open(input_json_path, \"w\") as f:\n        json.dump(input_json, f, indent=4)\n\n\ndef run_ptx(target_id, sequence, configs, solution, template_idx, runner):\n    temp_dir = f\"./{configs.dump_dir}/input\"\n    output_dir = f\"./{configs.dump_dir}/output\"\n    os.makedirs(temp_dir, exist_ok=True)\n    os.makedirs(output_dir, exist_ok=True)\n    process_sequence(sequence=sequence, target_id=target_id, temp_dir=temp_dir)\n    configs.input_json_path = os.path.join(temp_dir, f\"{target_id}_input.json\")\n    configs.template_idx = int(template_idx)\n    infer_predict(runner, configs)\n\n    # Collect ALL sample CIF files from all seeds (sorted by ranking score)\n    base_dir = f\"{configs.dump_dir}/{target_id}\"\n    collected = 0\n    for seed_dir in sorted(glob_mod.glob(f\"{base_dir}/seed_*\")):\n        pred_dir = f\"{seed_dir}/predictions\"\n        cif_files = sorted(glob_mod.glob(f\"{pred_dir}/{target_id}_sample_*.cif\"))\n        for cif_file in cif_files:\n            coord = extract_c1_coordinates(cif_file)\n            if coord is None:\n                coord = np.zeros((len(sequence), 3), dtype=np.float32)\n            elif coord.shape[0] < len(sequence):\n                pad = np.zeros((len(sequence) - coord.shape[0], 3), dtype=np.float32)\n                coord = np.concatenate([coord, pad], axis=0)\n            elif coord.shape[0] > len(sequence):\n                coord = coord[:len(sequence)]\n            solution[target_id].coord.append(coord)\n            collected += 1\n    print(f\"    Collected {collected} structures for {target_id} (template_idx={template_idx})\")\n\n    # Clean up CIF files to save disk space\n    if os.path.exists(base_dir):\n        shutil.rmtree(base_dir, ignore_errors=True)\n\n\ndef run():\n    LOG_FORMAT = \"%(asctime)s,%(msecs)-3d %(levelname)-8s [%(filename)s:%(lineno)s %(funcName)s] %(message)s\"\n    logging.basicConfig(format=LOG_FORMAT, level=logging.WARNING, datefmt=\"%Y-%m-%d %H:%M:%S\", filemode=\"w\")\n    logging.getLogger(\"rnapro.data\").setLevel(logging.WARNING)\n    logging.getLogger(\"rnapro\").setLevel(logging.WARNING)\n    configs_base[\"use_deepspeed_evo_attention\"] = (\n        os.environ.get(\"USE_DEEPSPEED_EVO_ATTENTION\", False) == \"true\"\n    )\n    configs = {**configs_base, **{\"data\": data_configs}, **inference_configs}\n    configs = parse_configs(configs=configs, arg_str=parse_sys_args(), fill_required_with_null=True)\n\n    # Parse seeds - handle comma-separated string from shell\n    seeds = configs.seeds\n    if isinstance(seeds, str):\n        if ',' in seeds:\n            seeds = [int(s.strip()) for s in seeds.split(',') if s.strip().isdigit()]\n        else:\n            seeds = [int(seeds)]\n    elif isinstance(seeds, (int, float)):\n        seeds = [int(seeds)]\n    elif not isinstance(seeds, (list, tuple)):\n        seeds = [42]\n    configs.seeds = seeds\n\n    valid_df = pd.read_csv(configs.sequences_csv)\n    print(f\"\\n -> Loaded {len(valid_df)} sequence(s), seeds={configs.seeds}\")\n\n    n_template_combos = configs.n_template_combos\n    print(f\" -> Using {n_template_combos} template combination(s)\")\n\n    print('\\n -> Building model and loading checkpoint')\n    runner = InferenceRunner(configs)\n    print('\\n -> Done, starting inference...')\n\n    solution = make_dummy_solution(valid_df)\n    for idx, row in valid_df.iterrows():\n        seq_len = len(row.sequence)\n        print(f\"\\n -> Sequence {row.target_id}: len={seq_len}\")\n        if seq_len > configs.max_len:\n            print(f'Sequence too long ({seq_len} > {configs.max_len}), skipping')\n            for _ in range(5):\n                solution[row.target_id].coord.append(\n                    np.zeros((seq_len, 3), dtype=np.float32)\n                )\n            continue\n        try:\n            for template_idx in [4]:  # Use ALL 5 templates at once\n                print(f'  template_combo={template_idx}')\n                run_ptx(\n                    target_id=row.target_id, sequence=row.sequence,\n                    configs=configs, solution=solution,\n                    template_idx=template_idx, runner=runner,\n                )\n        except Exception as e:\n            print(f\"Error processing {row.target_id}: {e}\")\n            traceback.print_exc()\n\n        # Cap at 5 predictions (best are first since CIFs are sorted by ranking score)\n        coords = solution[row.target_id].coord\n        if len(coords) == 0:\n            print(f\"  WARNING: No predictions for {row.target_id}, using zeros\")\n            for _ in range(5):\n                coords.append(np.zeros((seq_len, 3), dtype=np.float32))\n        while len(coords) < 5:\n            coords.append(coords[-1].copy())\n        solution[row.target_id].coord = coords[:5]\n        print(f\"  Final: {len(solution[row.target_id].coord)} predictions\")\n\n    print('\\n\\n -> Inference done! Saving to submission.csv')\n    submit_df = solution_to_submit_df(solution)\n    submit_df = submit_df.fillna(0.0)\n    submit_df.to_csv(\"./submission.csv\", index=False)\n\n\nif __name__ == \"__main__\":\n    run()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%writefile /kaggle/working/RNAPro/rnapro_inference_kaggle.sh\n\nexport LAYERNORM_TYPE=torch\n\n# === SUB 8: N_CYCLE=4 + multi-seed + all templates + RibonanzaNet2 ===\n# N_CYCLE=4 is RNAPro default, 2.5x faster than N_CYCLE=10\n# 3 seeds x 5 samples = 15 candidates, best 5 selected by ranking_score\nSEED=42,101,202\nN_SAMPLE=5\nN_STEP=200\nN_CYCLE=4\nN_TEMPLATE_COMBOS=1\n\n# Paths\nDUMP_DIR=\"../output\"\nCHECKPOINT_PATH=\"../rnapro-private-best-500m.ckpt\"\n\n# Template/MSA settings\nTEMPLATE_DATA=\"./release_data/kaggle/templates.pt\"\nRNA_MSA_DIR=\"/kaggle/input/stanford-rna-3d-folding-2/MSA/\"\nSEQUENCES_CSV=\"/kaggle/working/sample_sequences.csv\"\n\n# RibonanzaNet2\nRIBONANZA_PATH=\"/kaggle/input/ribonanzanet2/pytorch/alpha/1/\"\nif [ -d \"$RIBONANZA_PATH\" ]; then\n    echo \"RibonanzaNet2 found\"\n    RIBONANZA_ARGS=\"--model.use_RibonanzaNet2 true --model.ribonanza_net_path ${RIBONANZA_PATH}\"\nelse\n    echo \"WARNING: RibonanzaNet2 NOT found\"\n    RIBONANZA_ARGS=\"--model.use_RibonanzaNet2 false\"\nfi\n\nMODEL_NAME=\"rnapro_base\"\nmkdir -p \"${DUMP_DIR}\"\n\npython3 runner/inference.py \\\n    --model_name \"${MODEL_NAME}\" \\\n    --seeds ${SEED} \\\n    --dump_dir \"${DUMP_DIR}\" \\\n    --load_checkpoint_path \"${CHECKPOINT_PATH}\" \\\n    --use_msa true \\\n    --use_template \"ca_precomputed\" \\\n    --model.use_template \"ca_precomputed\" \\\n    --model.template_embedder.n_blocks 2 \\\n    ${RIBONANZA_ARGS} \\\n    --template_data \"${TEMPLATE_DATA}\" \\\n    --rna_msa_dir \"${RNA_MSA_DIR}\" \\\n    --model.N_cycle ${N_CYCLE} \\\n    --sample_diffusion.N_sample ${N_SAMPLE} \\\n    --sample_diffusion.N_step ${N_STEP} \\\n    --load_strict true \\\n    --num_workers 0 \\\n    --triangle_attention \"torch\" \\\n    --triangle_multiplicative \"torch\" \\\n    --sequences_csv \"${SEQUENCES_CSV}\" \\\n    --max_len 1000 \\\n    --n_template_combos ${N_TEMPLATE_COMBOS}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /kaggle/working/RNAPro\n!bash ./rnapro_inference_kaggle.sh\n!mv submission.csv /kaggle/working/submission_rnapro.csv\n%cd /kaggle/working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section F: Merge RNAPro + TBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport os\n\ndf_tbm = pd.read_csv(\"/kaggle/working/submission_tbm.csv\")\n\n# Check if RNAPro produced output\nrnapro_path = \"/kaggle/working/submission_rnapro.csv\"\nif os.path.exists(rnapro_path):\n    df_rnapro = pd.read_csv(rnapro_path)\n    df_seqs = pd.read_csv(\"/kaggle/input/stanford-rna-3d-folding-2/test_sequences.csv\")\n\n    # Identify long targets (>1000 nt) where RNAPro skips (outputs zeros)\n    long_targets = df_seqs[df_seqs['sequence'].str.len() > 1000]['target_id'].values\n    print(f\"Long targets to replace with TBM (len > 1000): {len(long_targets)}\")\n\n    # Build mask for rows belonging to long targets\n    mask_long = df_rnapro['ID'].apply(lambda x: any(str(x).startswith(t + \"_\") for t in long_targets))\n\n    if mask_long.sum() > 0:\n        print(f\"Replacing {mask_long.sum()} residues with TBM predictions...\")\n        df_rnapro_idx = df_rnapro.set_index('ID')\n        df_tbm_idx = df_tbm.set_index('ID')\n        ids_to_update = df_rnapro_idx[mask_long.values].index\n        valid_ids = [i for i in ids_to_update if i in df_tbm_idx.index]\n        df_rnapro_idx.loc[valid_ids] = df_tbm_idx.loc[valid_ids]\n        df_final = df_rnapro_idx.reset_index()\n    else:\n        print(\"No long targets to replace.\")\n        df_final = df_rnapro\nelse:\n    print(\"WARNING: RNAPro inference failed. Using TBM-only submission.\")\n    df_final = df_tbm\n\ndf_final.to_csv(\"/kaggle/working/submission.csv\", index=False)\nprint(f\"Final submission shape: {df_final.shape}\")\ndf_final.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section G: Validation (dev runs only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IS_SCORING_RUN:\n    print(\"Dev run - checking first 5 sequences only\")\n    sub = pd.read_csv('/kaggle/working/submission.csv')\n    print(f\"Submission shape: {sub.shape}\")\n    print(sub.head(10))\n    print(\"\\nNon-zero coordinate check:\")\n    coord_cols = [c for c in sub.columns if c.startswith(('x_', 'y_', 'z_'))]\n    for col in coord_cols[:3]:\n        nz = (sub[col] != 0).sum()\n        print(f\"  {col}: {nz}/{len(sub)} non-zero\")"
   ]
  }
 ]
}